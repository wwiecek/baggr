% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/baggr.R
\name{baggr}
\alias{baggr}
\title{Bayesian aggregate treatment effects model}
\usage{
baggr(
  data,
  model = NULL,
  pooling = c("partial", "none", "full"),
  effect_label = NULL,
  covariates = c(),
  prior_hypermean = NULL,
  prior_hypersd = NULL,
  prior_hypercor = NULL,
  prior_beta = NULL,
  prior_cluster = NULL,
  prior_control = NULL,
  prior_control_sd = NULL,
  prior_selection = NULL,
  prior_sigma = NULL,
  prior = NULL,
  ppd = FALSE,
  pooling_control = c("none", "partial", "remove"),
  test_data = NULL,
  quantiles = seq(0.05, 0.95, 0.1),
  outcome = "outcome",
  group = "group",
  treatment = "treatment",
  cluster = NULL,
  selection = NULL,
  silent = FALSE,
  warn = TRUE,
  ...
)
}
\arguments{
\item{data}{data frame with summary or individual level data to meta-analyse;
see Details section for how to format your data}

\item{model}{if \code{NULL}, detected automatically from input data
otherwise choose from \code{rubin}, \code{rubin_full}, \code{logit}, \code{mutau} (see Details).}

\item{pooling}{Type of pooling;
choose from \code{"none"}, \code{"partial"} (default) and \code{"full"}.
If you are not familiar with the terms, consult the vignette;
"partial" can be understood as random effects and "full" as fixed effects}

\item{effect_label}{How to label the effect(s). These labels are used in various print and plot outputs.
Will default to \code{"mean"} in most models, \code{"log OR"} in logistic model etc.
If you plan on comparing models (see \link{baggr_compare}), use the same labels.}

\item{covariates}{Character vector with column names in \code{data}. The corresponding columns are used as
covariates (fixed effects) in the meta-regression model (in case of aggregate data).
In the case of individual level data the model does not differentiate between group-level
variables (same values of the covariate for all rows related to a given group) and
individual-level covariates.}

\item{prior_hypermean}{prior distribution for hypermean; you can use "plain text" notation like
\code{prior_hypermean=normal(0,100)} or \code{uniform(-10, 10)}.
See \emph{Details:Priors} section below for more possible specifications.
If unspecified, the priors will be derived automatically based on data
(and printed out in the console).}

\item{prior_hypersd}{prior for hyper-standard deviation, used
by Rubin and \code{"mutau"} models;
same rules apply as for \verb{_hypermean};}

\item{prior_hypercor}{prior for hypercorrelation matrix, used by the \code{"mutau"} model}

\item{prior_beta}{prior for regression coefficients if \code{covariates} are specified; will default to
experimental normal(0, 10^2) distribution}

\item{prior_cluster}{priors for SDs of cluster random effects in each study
(i.e. assuming normal(0, sigma_k^2), with different sigma in each \code{group})}

\item{prior_control}{prior for the mean in the control arm (baseline), currently
used in \code{"logit"} model only;
if \code{pooling_control = "partial"}, the prior is hyperprior
for all baselines, if \code{"none"},
then it is an independent prior for all baselines}

\item{prior_control_sd}{prior for the SD in the control arm (baseline), currently
used in \code{"logit"} model only;
this can only be used if \code{pooling_control = "partial"}}

\item{prior_selection}{prior for the log of relative publication probability (see \strong{Selection} section below and argument \code{selection})}

\item{prior_sigma}{prior for error terms in linear regression models (\code{"rubin_full"} or \code{"mutau_full"})}

\item{prior}{alternative way to specify all of the priors above as a single named list, with \code{hypermean},
\code{hypersd}, \code{hypercor}, \code{beta} etc., with names dropping \code{prior_} prefix,
e.g. \code{prior = list(hypermean = normal(0,10), beta = uniform(-5, 5))}}

\item{ppd}{logical; use prior predictive distribution? (\emph{p.p.d.})
If \code{ppd=TRUE}, Stan model will sample from the prior distribution(s)
and ignore \code{data} in inference. However, \code{data} argument might still
be used to infer the correct model (if \code{model=NULL}) and to set the
default priors, therefore you must specify it.}

\item{pooling_control}{Pooling for group-specific control mean terms in models using
individual-level data. Typically we use either \code{"none"} or \code{"partial"},
but if you want to remove the group-specific intercept altogether,
set this to \code{"remove"}.}

\item{test_data}{data for cross-validation; NULL for no validation, otherwise a data frame
with the same columns as \code{data} argument. See "Cross-validation" section below.}

\item{quantiles}{if \code{model = "quantiles"}, a vector indicating which quantiles of data to use
(with values between 0 and 1)}

\item{outcome}{column name in \code{data} (used in individual-level only) with outcome variable values}

\item{group}{column name in \code{data} with grouping factor;
it's necessary for individual-level data, for summarised data
it will be used as labels for groups when displaying results}

\item{treatment}{column name in (individual-level) \code{data} with treatment factor;}

\item{cluster}{optional; column name in (individual-level) \code{data}; if defined,
random cluster effects will be fitted in each study}

\item{selection}{optional vector of z-values which implements symmetrical selection (relative probability of publication) model;
most commonly you'd set this to 1.96 (p=0.05) of \code{c(1.96, 2.58)} (p=0.05 and p=0.01)}

\item{silent}{Whether to silence messages about prior settings and about other automatic behaviour.}

\item{warn}{print an additional warning if Rhat exceeds 1.05}

\item{...}{extra options passed to Stan function, e.g. \code{control = list(adapt_delta = 0.99)},
number of iterations etc.}
}
\value{
\code{baggr} class structure: a list including Stan model fit
alongside input data, pooling metrics, various model properties.
If test data is used, mean value of -2*lpd is reported as \code{mean_lpd}
}
\description{
Bayesian inference on parameters of an average treatment effects model
that's appropriate to the supplied
individual- or group-level data, using Hamiltonian Monte Carlo in Stan.
(For overall package help file see \link{baggr-package})
}
\details{
Below we briefly discuss 1/ data preparation, 2/ choice of model, 3/ choice of priors.
All three are discussed in more depth in the package vignette, \code{vignette("baggr")}.

\strong{Data.} For aggregate data models you need a data frame with columns
\code{tau} and \code{se} (Rubin model) or \code{tau}, \code{mu}, \code{se.tau}, \code{se.mu} ("mu & tau" model).
An additional column can be used to provide labels for each group
(by default column \code{group} is used if available, but this can be
customised -- see the example below).
For individual level data three columns are needed: outcome, treatment, group. These
are identified by using the \code{outcome}, \code{treatment} and \code{group} arguments.

Many data preparation steps can be done through a helper function \link{prepare_ma}.
It can convert individual to summary-level data, calculate
odds/risk ratios (with/without corrections) in binary data, standardise variables and more.
Using it will automatically format data inputs to work with \code{baggr()}.

\strong{Models.} Available models are:
\itemize{
\item for the \strong{continuous variable} means:
\code{"rubin"} model for average treatment effect (using summary data), \code{"mutau"}
version which takes into account means of control groups (also using summary data),
\code{"rubin_full"},  which is the same model as \code{"rubin"} but works with individual-level data
\item for \strong{binary data}: \code{"logit"} model can be used on individual-level data;
you can also analyse continuous statistics such as
log odds ratios and logs risk ratios using the models listed above;
see \code{vignette("baggr_binary")} for tutorial with examples
}

If no model is specified, the function tries to infer the appropriate
model automatically.
Additionally, the user must specify type of pooling.
The default is always partial pooling.

\strong{Covariates.}
Both aggregate and individual-level data can include extra columns, given by \code{covariates} argument
(specified as a character vector of column names) to be used in regression models.
We also refer to impact of these covariates as \emph{fixed effects}.

Two types of covariates may be present in your data:
\itemize{
\item In \code{"rubin"} and \code{"mutau"} models, covariates that \strong{change according to group unit}.
In that case, the model accounting
for the group covariates is a
\href{https://handbook-5-1.cochrane.org/chapter_9/9_6_4_meta_regression.htm}{meta-regression}
model. It can be modelled on summary-level data.
\item In \code{"logit"} and \code{"rubin_full"} models, covariates that \strong{change according to individual unit}.
Then, such a model is often referred to as a mixed model. It has to be
fitted to individual-level data. Note that meta-regression is a special
case of a mixed model for individual-level data.
}

\strong{Priors.} It is optional to specify priors yourself,
as the package will try propose an appropriate
prior for the input data if you do not pass a \code{prior} argument.
To set the priors yourself, use \code{prior_} arguments. For specifying many priors at once
(or re-using between models), a single \code{prior = list(...)} argument can be used instead.
Meaning of the prior parameters may slightly change from model to model.
Details and examples are given in \code{vignette("baggr")}.
Setting \code{ppd=TRUE} can be used to obtain prior predictive distributions,
which is useful for understanding the prior assumptions,
especially useful in conjunction with \link{effect_plot}. You can also \link{baggr_compare}
different priors by setting \code{baggr_compare(..., compare="prior")}.

\strong{Cross-validation.} When \code{test_data} are specified, an extra parameter, the
log predictive density, will be returned by the model.
(The fitted model itself is the same regardless of whether there are \code{test_data}.)
To understand this parameter, see documentation of \link{loocv}, a function that
can be used to assess out of sample prediction of the model using all available data.
If using individual-level data model, \code{test_data} should only include treatment arms
of the groups of interest. (This is because in cross-validation we are not typically
interested in the model's ability to fit heterogeneity in control arms, but
only heterogeneity in treatment arms.)
For using aggregate level data, there is no such restriction.

\strong{Selection model.} If the \code{selection} argument is not \code{NULL}, \code{baggr()} fits a symmetric
selection-on-z-values model (currently only in the \code{"rubin"} summary-data model).
The values in \code{selection} are cut-points on |z| = |tau / se|; for example,
\code{selection = c(1.96, 2.58)} gives three intervals, \verb{[0, 1.96)}, \verb{[1.96, 2.58)},
and \verb{[2.58, Inf)}. Note how inequality works: 1.96 is "already" treated as more significant than 1.95.
(You should also consider defining it to more significant digits in large datasets.)
Each interval has its own relative publication probability
(weight), with the highest-|z| interval normalised to 1, and these weights
are estimated jointly with the usual Rubin parameters.

The selection model assumes that publication depends only on |z| (not on the
sign or other study features). Inference can be very sensitive to the choice of
cut-points and priors on the selection weights, which you should set manually.
For more complex cases you should consider using other methods.

\strong{Outputs.} By default, some outputs are printed. There is also a
plot method for \emph{baggr} objects which you can access via \link{baggr_plot} (or simply \code{plot()}).
Other standard functions for working with \code{baggr} object are
\itemize{
\item \link{treatment_effect} for distribution of hyperparameters
\item \link{group_effects} for distributions of group-specific parameters (alias: \link{study_effects}, we use the two interchangeably)
\item \link{fixed_effects} for coefficients in (meta-)regression
\item \link{effect_draw} and \link{effect_plot} for posterior predictive distributions
\item \link{baggr_compare} for comparing multiple \code{baggr} models
\item \link{loocv} for cross-validation
}
}
\examples{
df_pooled <- data.frame("tau" = c(1, -1, .5, -.5, .7, -.7, 1.3, -1.3),
                        "se" = rep(1, 8),
                        "state" = datasets::state.name[1:8])
baggr(df_pooled) #baggr automatically detects the input data
# same model, but with correct labels,
# different pooling & passing some options to Stan
baggr(df_pooled, group = "state", pooling = "full", iter = 500)
# model with non-default (and very informative) priors

baggr(df_pooled, prior_hypersd = normal(0, 2))

\donttest{
# "mu & tau" model, using a built-in dataset
# prepare_ma() can summarise individual-level data
ms <- microcredit_simplified
microcredit_summary_data <- prepare_ma(ms, outcome = "consumption")
baggr(microcredit_summary_data, model = "mutau",
      iter = 500, #this is just for illustration -- don't set it this low normally!
      pooling = "partial", prior_hypercor = lkj(1),
      prior_hypersd = normal(0,10),
      prior_hypermean = multinormal(c(0,0),matrix(c(10,3,3,10),2,2)))
}


}
