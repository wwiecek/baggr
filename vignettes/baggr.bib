
@article{gelman_understanding_2014,
  title = {Understanding Predictive Information Criteria for {{Bayesian}} Models},
  volume = {24},
  issn = {0960-3174, 1573-1375},
  abstract = {We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected out-of-sample-prediction error using a biascorrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this review is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.},
  language = {en},
  number = {6},
  journal = {Statistics and Computing},
  doi = {10.1007/s11222-013-9416-2},
  author = {Gelman, Andrew and Hwang, Jessica and Vehtari, Aki},
  month = nov,
  year = {2014},
  pages = {997-1016},
  file = {C\:\\Users\\wwiecek.CERTARA\\Zotero\\storage\\KDSC9Z5I\\Gelman et al. - 2014 - Understanding predictive information criteria for .pdf},
  note = {00642}
}

@article{gelman_bayesian_2006,
  title = {Bayesian {{Measures}} of {{Explained Variance}} and {{Pooling}} in {{Multilevel}} ({{Hierarchical}}) {{Models}}},
  volume = {48},
  issn = {0040-1706, 1537-2723},
  language = {en},
  number = {2},
  journal = {Technometrics},
  doi = {10.1198/004017005000000517},
  author = {Gelman, Andrew and Pardoe, Iain},
  month = may,
  year = {2006},
  pages = {241-251},
  file = {C\:\\Users\\wwiecek.CERTARA\\Zotero\\storage\\4ZR2BXTF\\Gelman and Pardoe - 2006 - Bayesian Measures of Explained Variance and Poolin.pdf},
  note = {00000}
}

@book{gelman_bayesian_2013,
  title = {Bayesian {{Data Analysis}}},
  isbn = {978-1-4398-9820-8},
  abstract = {Winner of the 2016 De Groot Prize from the International Society for Bayesian Analysis Now in its third edition, this classic book is widely considered the leading text on Bayesian methods, lauded for its accessible, practical approach to analyzing data and solving research problems. Bayesian Data Analysis, Third Edition continues to take an applied approach to analysis using up-to-date Bayesian methods. The authors\textemdash{}all leaders in the statistics community\textemdash{}introduce basic concepts from a data-analytic perspective before presenting advanced methods. Throughout the text, numerous worked examples drawn from real applications and research emphasize the use of Bayesian inference in practice. New to the Third Edition   Four new chapters on nonparametric modeling Coverage of weakly informative priors and boundary-avoiding priors Updated discussion of cross-validation and predictive information criteria Improved convergence monitoring and effective sample size calculations for iterative simulation Presentations of Hamiltonian Monte Carlo, variational Bayes, and expectation propagation New and revised software code   The book can be used in three different ways. For undergraduate students, it introduces Bayesian inference starting from first principles. For graduate students, the text presents effective current approaches to Bayesian modeling and computation in statistics and related fields. For researchers, it provides an assortment of Bayesian methods in applied statistics. Additional materials, including data sets used in the examples, solutions to selected exercises, and software instructions, are available on the book's web page.},
  language = {en},
  publisher = {{CRC Press}},
  author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
  month = nov,
  year = {2013},
  keywords = {Mathematics / Probability \& Statistics / General,Psychology / Research \& Methodology}
}

@article{rubin_estimation_1981,
  title = {Estimation in {{Parallel Randomized Experiments}}},
  volume = {6},
  issn = {0362-9791},
  abstract = {[Many studies comparing new treatments to standard treatments consist of parallel randomized experiments. In the example considered here, randomized experiments were conducted in eight schools to determine the effectiveness of special coaching programs for the SAT. The purpose here is to illustrate Bayesian and empirical Bayesian techniques that can be used to help summarize the evidence in such data about differences among treatments, thereby obtaining improved estimates of the treatment effect in each experiment, including the one having the largest observed effect. Three main tools are illustrated: 1) graphical techniques for displaying sensitivity within an empirical Bayes framework, 2) simple simulation techniques for generating Bayesian posterior distributions of individual effects and the largest effect, and 3) methods for monitoring the adequacy of the Bayesian model specification by simulating the posterior predictive distribution in hypothetical replications of the same treatments in the same eight schools.]},
  number = {4},
  journal = {Journal of Educational Statistics},
  author = {Rubin, Donald B.},
  year = {1981},
  pages = {377-401}
}

@article{meager_aggregating_2019-1,
  title = {Aggregating {{Distributional Treatment Effects}}: {{A Bayesian Hierarchical Analysis}} of the {{Microcredit Literature}}},
  shorttitle = {Aggregating {{Distributional Treatment Effects}}},
  abstract = {This paper develops methods to aggregate evidence on distributional treatment effects from multiple studies conducted in different settings, and applies them to the microcredit literature. Several randomized trials of expanding access to microcredit found substantial effects on the tails of household outcome distributions, but the extent to which these findings generalize to future settings was not known. Aggregating the evidence on sets of quantile effects poses additional challenges relative to average effects because distributional effects must imply monotonic quantiles and pass information across quantiles. Using a Bayesian hierarchical framework, I develop new models to aggregate distributional effects and assess their generalizability. For continuous outcome variables, the methodological challenges are addressed by applying transforms to the unknown parameters. For partially discrete variables such as business profits, I use contextual economic knowledge to build tailored parametric aggregation models. I find generalizable evidence that microcredit has negligible impact on the distribution of various household outcomes below the 75th percentile, but above this point there is no generalizable prediction. Thus, while microcredit typically does not lead to worse outcomes at the group level, there is no generalizable evidence on whether it improves group outcomes. Households with previous business experience account for the majority of the impact and see large increases in the right tail of the consumption distribution.},
  language = {en},
  doi = {10.31222/osf.io/7tkvm},
  author = {Meager, Rachael},
  year = {2019},
  file = {C\:\\Users\\wwiecek.CERTARA\\Zotero\\storage\\EVR5X7SE\\Meager - Aggregating Distributional Treatment Effects A Ba.pdf}
}

@article{rubin_estimating_1974,
  title = {Estimating {{Causal Effects}} of {{Treatments}} in {{Randomized}} and {{Nonrandomized Studies}}},
  copyright = {closed access},
  issn = {0022-0663},
  abstract = {Presents a discussion of matching, randomization, random sampling, and other methods of controlling extraneous variation. The objective was to specify the benefits of randomization in estimating causal effects of treatments. It is concluded that randomization should be employed whenever possible but that the use of carefully controlled nonrandomized data to estimate causal effects is a reasonable and necessary procedure in many cases.},
  language = {en\_US},
  journal = {Journal of Educational Psychology},
  doi = {10.1037/h0037350},
  author = {Rubin, Donald B.},
  year = {1974},
  file = {C\:\\Users\\wwiecek.CERTARA\\Zotero\\storage\\YU5492PB\\3408692.html}
}

@article{firpo_efficient_2007,
  title = {Efficient {{Semiparametric Estimation}} of {{Quantile Treatment Effects}}},
  volume = {75},
  issn = {0012-9682},
  abstract = {This paper develops estimators for quantile treatment effects under the identifying restriction that selection to treatment is based on observable characteristics. Identification is achieved without requiring computation of the conditional quantiles of the potential outcomes. Instead, the identification results for the marginal quantiles lead to an estimation procedure for the quantile treatment effect parameters that has two steps: nonparametric estimation of the propensity score and computation of the difference between the solutions of two separate minimization problems. Root-N consistency, asymptotic normality, and achievement of the semiparametric efficiency bound are shown for that estimator. A consistent estimation procedure for the variance is also presented. Finally, the method developed here is applied to evaluation of a job training program and to a Monte Carlo exercise. Results from the empirical application indicate that the method works relatively well even for a data set with limited overlap between treated and controls in the support of covariates. The Monte Carlo study shows that, for a relatively small sample size, the method produces estimates with good precision and low bias, especially for middle quantiles.},
  number = {1},
  journal = {Econometrica},
  author = {Firpo, Sergio},
  year = {2007},
  pages = {259-276}
}

@article{mcculloch_misspecifying_2011,
  title = {Misspecifying the {{Shape}} of a {{Random Effects Distribution}}: {{Why Getting It Wrong May Not Matter}}},
  volume = {26},
  issn = {0883-4237, 2168-8745},
  shorttitle = {Misspecifying the {{Shape}} of a {{Random Effects Distribution}}},
  abstract = {Statistical models that include random effects are commonly used to analyze longitudinal and correlated data, often with strong and parametric assumptions about the random effects distribution. There is marked disagreement in the literature as to whether such parametric assumptions are important or innocuous. In the context of generalized linear mixed models used to analyze clustered or longitudinal data, we examine the impact of random effects distribution misspecification on a variety of inferences, including prediction, inference about covariate effects, prediction of random effects and estimation of random effects variances. We describe examples, theoretical calculations and simulations to elucidate situations in which the specification is and is not important. A key conclusion is the large degree of robustness of maximum likelihood for a wide variety of commonly encountered situations.},
  language = {EN},
  number = {3},
  journal = {Statistical Science},
  doi = {10.1214/11-STS361},
  author = {McCulloch, Charles E. and Neuhaus, John M.},
  month = aug,
  year = {2011},
  keywords = {Maximum likelihood,mixed models,parametric modeling},
  pages = {388-402},
  file = {C\:\\Users\\wwiecek.CERTARA\\Zotero\\storage\\PSYLLUNM\\McCulloch and Neuhaus - 2011 - Misspecifying the Shape of a Random Effects Distri.pdf;C\:\\Users\\wwiecek.CERTARA\\Zotero\\storage\\9JRP8YBX\\1320066927.html}
}


